#LyX 2.0.0beta1 created this file. For more info see http://www.lyx.org/
\lyxformat 407
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_xetex false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Data Mining Classification Models: an analysis
\end_layout

\begin_layout Author
Daniel Hanspeter 6129, Daniel Graziotin 4801, Thomas Schievenin 5701
\end_layout

\begin_layout Date
October / November 2010
\end_layout

\begin_layout Abstract
This document analyzes various techniques of analysis and data manipulation.
 In particular, it investigates about four different classification models,
 namely Decision tree, NaÃ¯ve Bayesian Classifier, Logistic Regression and
 KNN classifier, that are summarized in the last page.
 The experiments are performed on two completely different datasets, blood-trans
fusion and census-income, specifically chosen in order to compare work done
 on small versus big datasets.
 Blood transfusion is relatively small and has a reduced amount of attributes,
 while census-income is bigger, having also a more relevant amount of attributes
 to investigate on.
 On the first dataset there is not a class so we introduced one, in order
 to perform our test and focus on classification and prediction.
 This was not needed for the second dataset, since it was already containing
 a class to be analyzed.
 Therefore we used this dataset mainly for benchmarking and performance
 analysis of the classification models by varying the data pre-processing
 techniques they have in common.
\end_layout

\begin_layout Section
Data Examination
\end_layout

\begin_layout Subsection
Blood Transfusion
\end_layout

\begin_layout Subsection
Census Income
\end_layout

\begin_layout Standard
The dataset contains 15 attributes and a total of 32561 instance.
 The following is a representation of the attributes and their types, extracted
 from the ARFF file we created.
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "basicstyle={\scriptsize}"
inline false
status open

\begin_layout Plain Layout

age NUMERIC 
\end_layout

\begin_layout Plain Layout

workclass {Private, Self-emp-not-inc, Federal-gov, [..], Never-worked}  
\end_layout

\begin_layout Plain Layout

fnlwgt NUMERIC
\end_layout

\begin_layout Plain Layout

education {Bachelors, Some-college, [..], Preschool}  
\end_layout

\begin_layout Plain Layout

education-num NUMERIC  
\end_layout

\begin_layout Plain Layout

marital-status {Married-civ-spouse, Divorced, [..], Married-AF-spouse}
\end_layout

\begin_layout Plain Layout

occupation {Tech-support, Craft-repair, [..], Armed-Forces}  
\end_layout

\begin_layout Plain Layout

relationship {Wife, Own-child, Husband, [..], Unmarried}  
\end_layout

\begin_layout Plain Layout

race	 {White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black} 
\end_layout

\begin_layout Plain Layout

sex 	{Female, Male}  
\end_layout

\begin_layout Plain Layout

capital-gain NUMERIC
\end_layout

\begin_layout Plain Layout

capital-loss NUMERIC
\end_layout

\begin_layout Plain Layout

hours-per-week NUMERIC
\end_layout

\begin_layout Plain Layout

native-country {United-States, Cambodia, England, [..]}  
\end_layout

\begin_layout Plain Layout

class 	{ >50K, <=50K}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Education
\series default
 and 
\series bold
rducation-num
\series default
 are referring to the same value, where education-num is a numerical representat
ion of education.
\end_layout

\begin_layout Standard
Attribute 
\series bold
fnlwgt
\series default
 does not appear to have any meaningful sense.
 Further researches confirmed our 
\end_layout

\begin_layout Standard
There are 
\series bold
missing values
\series default
 for attributes workclass (6%), occupation (6%), native-country (2%).
 Two possible strategies to deal with the missing values are complete removal
 of the instances having missing values and substitution of the missing
 values with the mode value of the attribute.
 
\end_layout

\begin_layout Standard
The two attributes age and hours-per-week have been selected for further
 analysis.
 The following are their 
\series bold
boxplot
\series default
 representation:
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename img/census-distribution_age.jpg
	scale 45

\end_inset


\end_layout

\end_inset


\begin_inset space \space{}
\end_inset


\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "45col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename img/census-distribution_hours_per_week.jpg
	scale 45

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
There are 
\series bold
outliers
\series default
 for both the selected attributes.
 They will be treated with discretization techniques.
\end_layout

\begin_layout Standard
Possible 
\series bold
data similarity
\series default
 technique: here we have in addition to numeric values also nominal values.
 In this case we can do a simple matching using the formula d(I,j)= (p-m)/p
 where m is the number of matches and p is the number of variables or we
 could create a binary mapping for those values.
\end_layout

\begin_layout Standard
Possibe data mining tasks: an interesting task could focus around the income
 class.
 For example we could construct a tree which tells us if a 20-24 or a 25-30
 years old with different work class can be mapped in a >50K income class.
\end_layout

\begin_layout Standard
E.G Age + Work => Class income: >50K /<=50K 
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename img/census-example_tree.jpg
	scale 80

\end_inset


\end_layout

\begin_layout Section
Analysis of Techniques
\end_layout

\begin_layout Subsection
Census Income
\end_layout

\begin_layout Subsection*
Benchmark Framework
\end_layout

\begin_layout Standard
For analyzing the classification models by varying the data pre-processing
 techniques, we decided to prepare an evaluation framework (that is, a benchmark
) including various possible combination of data pre-processing techniques.
 The following are the variables taken into account by preparing the framework.
\end_layout

\begin_layout Subsubsection*
Framework variables
\end_layout

\begin_layout Paragraph*
Missing Values
\end_layout

\begin_layout Standard
We decided to simply substitute the missing values of instances with the
 mode value of each attribute, as the method of removing instances is discourage
d.
 
\end_layout

\begin_layout Paragraph*
Outliers
\end_layout

\begin_layout Standard
Outliers will be treated by applying discretization.
\end_layout

\begin_layout Paragraph*
Attribute selection
\end_layout

\begin_layout Standard
We will just remove the two attributes that we find of no particular interest.
 No other attribute selection techniques are taken into account, as we don't
 find comfortable with them.
 
\end_layout

\begin_layout Paragraph*
Chosen Datasets
\end_layout

\begin_layout Standard
We will use the four classification models on the full data set.
\end_layout

\begin_layout Subsubsection*
Framework approach
\end_layout

\begin_layout Standard
The framework will be run as described in this paragraph.
 The four classification models will be run in this ways:
\end_layout

\begin_layout Enumerate
Without any preprocessing technique
\end_layout

\begin_layout Enumerate
By substituting missing values
\end_layout

\begin_layout Enumerate
By treating outliers
\end_layout

\begin_layout Enumerate
By treating outliers and substituting missing values
\end_layout

\begin_layout Enumerate
By removing fnlwgt and education-num
\end_layout

\begin_layout Enumerate
Removing fnlwgt, education-num, substituting missing values
\end_layout

\begin_layout Enumerate
Removing fnlwgt, education-num, treating outliers
\end_layout

\begin_layout Enumerate
Removing fnlwgt, education-num, substituting missing values, treating outliers
\end_layout

\begin_layout Standard
The best options for each classification algorithm will be chosen and used
 for further experiments.
\end_layout

\begin_layout Subsection*
Results
\end_layout

\begin_layout Standard
The following is the list of the results of the tests run using the configuratio
ns presented in the previous paragraph.
 We highlighted with different colors the casistics in which each algorithm
 performed better.
 We also enclosed in square brackets the best performing algorithm for each
 run.
 For steps 1, 2, 3 and 4 we also used different values for k parameter,
 as the results would not change otherwise, and keep the best found k for
 the next settings.
\end_layout

\begin_layout Enumerate
J48 87.8566%; NaiveBayes 83.4465%; Logistics 84.896%;
\color black
 K-Nearest Neighbour (k=400): 85.3482%
\end_layout

\begin_layout Enumerate

\color green
J48 88.0102%
\color inherit
; NaiveBayes 83.3144%; Logistics 84.896%;
\color black
 K-Nearest Neighbour: (k=10): 85.9065%
\end_layout

\begin_layout Enumerate
J48 87.8566%; 
\color magenta
NaiveBayes 83.4557%
\color inherit
; Logistics 84.896%
\series bold
; 
\series default
\color black
K-Nearest Neighbour (k=7): 86.8063%
\end_layout

\begin_layout Enumerate

\color green
J48 88.0102%
\color inherit
; NaiveBayes 83.2837%; Logistics 84.896%; 
\color blue
K-Nearest Neighbour (k=2): 89.9665%
\end_layout

\begin_layout Enumerate
J48 87.0858%; NaiveBayes 82.4729%; Logistics 85.1663%; K-Nearest Neighbour
 (k=2): 89.8222%
\end_layout

\begin_layout Enumerate
J48 87.1380%; NaiveBayes 82.3193%; Logistics 85.1663%; K-Nearest Neighbour
 (k=2): 89.856%
\end_layout

\begin_layout Enumerate
J48 85.1172%; NaiveBayes 81.1032%; 
\color red
Logistics 85.326%
\color inherit
; K-Nearest Neighbour (k=2): 87.1042%
\end_layout

\begin_layout Enumerate
J48 85.1172%; NaiveBayes 80.9588%; 
\color red
Logistics 85.326%
\color inherit
; K-Nearest Neighbour (k=2): 87.0643%
\end_layout

\begin_layout Standard
Delta accuracy results (best accuracy - worse accuracy):
\end_layout

\begin_layout Itemize
J48: 2,8930%
\end_layout

\begin_layout Itemize
NaiveBayes: 2,4969%
\end_layout

\begin_layout Itemize
Logistics: 0,4300%
\end_layout

\begin_layout Itemize
K-NN (k=2): 2,9022%
\end_layout

\begin_layout Section*
Conclusions
\end_layout

\begin_layout Standard
The choice of two very different data sets allowed us to reason about different
 aspects of data mining.
 A small data size is easier and faster to be analyzed but less realiable
 for studies.
 A bigger one is slow to be analyzed (especially with K-NN model) but gives
 more realiable results.
\end_layout

\begin_layout Standard
The benchmark on data preprocessing techniques we run on census-income dataset
 showed that the classification models react very differently.
 The accuracy range can be improved by circa 3%.
 Logistics Regression does not show improvements when varying data preprocessing
 techniques.
\end_layout

\begin_layout Standard
\begin_inset External
	template PDFPages
	filename /Users/bodom_lx/Projects/DWDM/ClassificationAlgorithms.pdf

\end_inset


\end_layout

\end_body
\end_document
